{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba1442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import NeuralNetwork, FC, VGG, VGG16, EfficientNetB0\n",
    "from Dataset import Dataset, SPIRAL, MOON, CIRCLE, MNIST, Fashion_MNIST, CIFAR10\n",
    "from NeuroEvolution import NeuroEvolution\n",
    "from tensorflow import keras\n",
    "from Representation import Function, PAU\n",
    "from Activation import PANGAEA_Activation, Pade_Activation_Unit\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from numpy import random\n",
    "from Swarm import Star, Swarm\n",
    "import numpy as np\n",
    "import copy \n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab7295c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e87902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19b2beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10\n",
    "\n",
    "\n",
    "dataset_train = dataset(training_split=.8)\n",
    "\n",
    "\n",
    "\n",
    "blocks = 3\n",
    "\n",
    "nn = VGG(dataset_train, blocks)\n",
    "\n",
    "metric = keras.metrics.CategoricalAccuracy()\n",
    "loss_noise = 0.\n",
    "activation_noise = 0.\n",
    "input_noise = 0.\n",
    "gradient_noise =  0.\n",
    "weight_noise = 6\n",
    "label_smoothing = 0.0\n",
    "gradient_dropout = 0.\n",
    "dropout = 0.0\n",
    "drop_connect = 0.\n",
    "batch_size = 128\n",
    "batch_schedule = 0.\n",
    "drnn = 0.\n",
    "weight_std =  0.05\n",
    "verbose = 1\n",
    "epochs_search = 20\n",
    "epochs_test = 100\n",
    "iterations = 100000000\n",
    "patience = np.inf\n",
    "save_best = False\n",
    "sleep = 0.\n",
    "cut_threshold = 0.12\n",
    "batch_range = [128, 1024]\n",
    "random_flip=1\n",
    "random_rotation=0.0\n",
    "random_zoom=0.0\n",
    "random_translation=0.0\n",
    "random_contrast=0.\n",
    "shuffle = 0\n",
    "lr = 3\n",
    "optimizer = 1\n",
    "lr_schedule = 0.0\n",
    "lr_range = [1, 5]\n",
    "\n",
    "\n",
    "\n",
    "nn.set_config(shuffle, random_flip, random_rotation, random_zoom, random_translation, random_contrast, input_noise,\n",
    "              label_smoothing, weight_std, dropout, drop_connect, drnn, activation_noise, loss_noise,\n",
    "              optimizer, lr, lr_schedule, batch_size, batch_schedule, weight_noise, gradient_noise, gradient_dropout,              \n",
    "              metric, epochs_search, iterations, patience, verbose=verbose,\n",
    "              batch_range=batch_range, lr_range=lr_range, sleep=sleep, save_best=save_best, cut_threshold=cut_threshold)\n",
    "nn.create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd931e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43eed512",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "methods = ['shuffle', 'flip', 'rotation', 'zoom', 'translation', 'contrast'\n",
    "                               , 'input noise' , 'label smoothing' , 'weight init', 'dropout', 'dropconnect',\n",
    "                               'drnn', 'activation noise', 'loss noise', \n",
    "                               'optimizer', 'lr', 'lr schedule', 'batch', 'batch schedule', 'weight noise', 'gradient noise', \n",
    "                               'gradient dropout']\n",
    "\n",
    "default_settings = [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 3, 0.0, 128, 0.0, 6.0, 0.0, 0.0]\n",
    "\n",
    "population_size = 140\n",
    "ind_size = 22\n",
    "options = {'c1': 1.49618, 'c2': 1.49618, 'w': 0.7298, 'k': population_size, 'p': 2}\n",
    "bounds = [[0.0, 1.], [0.0, 4.], [0.0, 0.2], [0.0, 0.2],  \n",
    "          [0.0, 0.2], [0.0, 0.2], [0.0, 1.], [0.0, 1.], \n",
    "          [0.0, 0.1], [0.0, 0.5], [0.0, 0.5], [0.0, 1.], \n",
    "          [0.0, 0.1], [0.0, 1.], [0.0, 3.0], nn.lr_range, \n",
    "          [-1.0, 1.0], nn.batch_range, [-1.0, 1.0], \n",
    "          [1.0, 6.0], [0.0, 1.0], [0.0, 0.50]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e56fe5",
   "metadata": {},
   "source": [
    "# Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8cd06e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss_noise: 0.0 , activation_noise: 0.0 , input_noise: 0.0 , label_smoothing: 0.0 , weight_noise: 0 , gradient_dropout: 0.0 , gradient_noise: 0.0 , batch_size: 128 , dropout: 0.0 , drop_connect: 0.0 , batch_schedule: 21 , drnn: 0.0 , weight_std: 0.05 , flip: 1 , rotation: 0.0 , zoom: 0.0 , translation: 0.0 , contrast: 0.0 , shuffle: 0 , lr: 0.001 , optimizer: 1 , lr_schedule: 21 , batch_increase: 0 , lr_increase: 0\n",
      "\n",
      "Start of epoch 0\n",
      "cuurent batch: 128\n",
      "Training acc: 0.4517\n",
      "Validation acc: 0.4309\n",
      "Time taken: 8.78s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 1\n",
      "cuurent batch: 128\n",
      "Training acc: 0.6120\n",
      "Validation acc: 0.6047\n",
      "Time taken: 8.20s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 2\n",
      "cuurent batch: 128\n",
      "Training acc: 0.6961\n",
      "Validation acc: 0.6404\n",
      "Time taken: 8.38s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 3\n",
      "cuurent batch: 128\n",
      "Training acc: 0.7564\n",
      "Validation acc: 0.6861\n",
      "Time taken: 8.02s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 4\n",
      "cuurent batch: 128\n",
      "Training acc: 0.8081\n",
      "Validation acc: 0.7165\n",
      "Time taken: 8.40s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 5\n",
      "cuurent batch: 128\n",
      "Training acc: 0.8523\n",
      "Validation acc: 0.7091\n",
      "Time taken: 8.00s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 6\n",
      "cuurent batch: 128\n",
      "Training acc: 0.8852\n",
      "Validation acc: 0.7071\n",
      "Time taken: 8.14s\n",
      "Patience:  2\n",
      "\n",
      "Start of epoch 7\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9089\n",
      "Validation acc: 0.7242\n",
      "Time taken: 8.39s\n",
      "Patience:  3\n",
      "\n",
      "Start of epoch 8\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9290\n",
      "Validation acc: 0.7225\n",
      "Time taken: 8.17s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 9\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9421\n",
      "Validation acc: 0.7300\n",
      "Time taken: 8.11s\n",
      "Patience:  2\n",
      "\n",
      "Start of epoch 10\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9546\n",
      "Validation acc: 0.7261\n",
      "Time taken: 8.24s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 11\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9632\n",
      "Validation acc: 0.7384\n",
      "Time taken: 8.28s\n",
      "Patience:  2\n",
      "\n",
      "Start of epoch 12\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9593\n",
      "Validation acc: 0.7342\n",
      "Time taken: 8.19s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 13\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9752\n",
      "Validation acc: 0.7401\n",
      "Time taken: 8.13s\n",
      "Patience:  2\n",
      "\n",
      "Start of epoch 14\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9832\n",
      "Validation acc: 0.7459\n",
      "Time taken: 8.34s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 15\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9866\n",
      "Validation acc: 0.7412\n",
      "Time taken: 8.40s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 16\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9902\n",
      "Validation acc: 0.7493\n",
      "Time taken: 8.46s\n",
      "Patience:  2\n",
      "\n",
      "Start of epoch 17\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9910\n",
      "Validation acc: 0.7534\n",
      "Time taken: 8.54s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 18\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9931\n",
      "Validation acc: 0.7519\n",
      "Time taken: 8.02s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 19\n",
      "cuurent batch: 128\n",
      "Training acc: 0.9951\n",
      "Validation acc: 0.7556\n",
      "Time taken: 8.10s\n",
      "Patience:  2\n",
      "0.7468000054359436\n",
      "0.7555999755859375 0.7468000054359436\n"
     ]
    }
   ],
   "source": [
    "fitness = NeuroEvolution.evaluate(default_settings, [nn], test_time=True, verbose=1)[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd29f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f48ca6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "cuurent batch: 626\n",
      "Training acc: 0.2818\n",
      "Validation acc: 0.1114\n",
      "Time taken: 6.34s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 0\n",
      "cuurent batch: 377\n",
      "Training acc: 0.1024\n",
      "Validation acc: 0.1014\n",
      "Time taken: 6.10s\n",
      "Patience:  1\n",
      "\n",
      "Start of epoch 0\n",
      "cuurent batch: 805\n",
      "Training acc: 0.2333\n",
      "Validation acc: 0.1020\n",
      "Time taken: 6.12s\n",
      "Patience:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m choice \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m17\u001b[39m, \u001b[38;5;241m18\u001b[39m]:\n\u001b[0;32m     18\u001b[0m                 swarm\u001b[38;5;241m.\u001b[39mposition[i][j] \u001b[38;5;241m=\u001b[39m default_settings[j]\n\u001b[1;32m---> 20\u001b[0m ost, pos, time_found \u001b[38;5;241m=\u001b[39m \u001b[43mswarm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_change\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Radnomness_in_Neural_Network\\Swarm.py:163\u001b[0m, in \u001b[0;36mSwarm.optimize\u001b[1;34m(self, model, steps, no_change, verbose)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# print(ind.shape)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 163\u001b[0m     fitness \u001b[38;5;241m=\u001b[39m \u001b[43mNeuroEvolution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    165\u001b[0m     computation_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(computation_df)] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39msum(ind), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start]\n\u001b[0;32m    167\u001b[0m     row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(ind), time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreation_time, fitness]\n",
      "File \u001b[1;32m~\\PycharmProjects\\Radnomness_in_Neural_Network\\NeuroEvolution.py:142\u001b[0m, in \u001b[0;36mNeuroEvolution.evaluate\u001b[1;34m(individual, models, reps, test_time, verbose, log)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m#evaluation_model.max_batch = model.max_batch\u001b[39;00m\n\u001b[0;32m    141\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 142\u001b[0m \u001b[43mevaluation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m training_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m evaluation_model\u001b[38;5;241m.\u001b[39mtraining_time\n\u001b[0;32m    144\u001b[0m inference_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m evaluation_model\u001b[38;5;241m.\u001b[39minference_time\n",
      "File \u001b[1;32m~\\PycharmProjects\\Radnomness_in_Neural_Network\\NeuralNetwork.py:145\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 145\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensor_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_trainSampled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_trainSampled\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle):\n\u001b[0;32m    147\u001b[0m         train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:793\u001b[0m, in \u001b[0;36mDatasetV2.from_tensor_slices\u001b[1;34m(tensors, name)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tensor_slices\u001b[39m(tensors, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    717\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a `Dataset` whose elements are slices of the given tensors.\u001b[39;00m\n\u001b[0;32m    718\u001b[0m \n\u001b[0;32m    719\u001b[0m \u001b[38;5;124;03m  The given tensors are sliced along their first dimension. This operation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    791\u001b[0m \u001b[38;5;124;03m    Dataset: A `Dataset`.\u001b[39;00m\n\u001b[0;32m    792\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTensorSliceDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4477\u001b[0m, in \u001b[0;36mTensorSliceDataset.__init__\u001b[1;34m(self, element, is_files, name)\u001b[0m\n\u001b[0;32m   4475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, element, is_files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   4476\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 4477\u001b[0m   element \u001b[38;5;241m=\u001b[39m \u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4478\u001b[0m   batched_spec \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mtype_spec_from_value(element)\n\u001b[0;32m   4479\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_batched_tensor_list(batched_spec, element)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:125\u001b[0m, in \u001b[0;36mnormalize_element\u001b[1;34m(element, element_signature)\u001b[0m\n\u001b[0;32m    122\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(spec, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    124\u001b[0m         normalized_components\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 125\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomponent_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mpack_sequence_as(pack_as, normalized_components)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1695\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor did not convert to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1691\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe preferred dtype: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1692\u001b[0m                       (ret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype, preferred_dtype\u001b[38;5;241m.\u001b[39mbase_dtype))\n\u001b[0;32m   1694\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1695\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1698\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     47\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m as_ref  \u001b[38;5;66;03m# Unused.\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "population_size = 10\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "star = Star(population_size)\n",
    "swarm = Swarm(star, population_size, ind_size, options, bounds, 0.2, bounded=True)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "for i in range(population_size):\n",
    "        choice = np.random.choice([1,2,3,4,5,6,7, 9, 10, 11, 12, 13, 19,20,21], 4, replace=False)\n",
    "        for j in range(ind_size):\n",
    "            if j not in choice and j not in [0, 8, 14, 15, 16, 17, 18]:\n",
    "                swarm.position[i][j] = default_settings[j]\n",
    "\n",
    "ost, pos, time_found = swarm.optimize(nn, steps=200, no_change=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness = NeuroEvolution.evaluate(pos, [nn], test_time=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b75fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3308acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721b273a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00569b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
